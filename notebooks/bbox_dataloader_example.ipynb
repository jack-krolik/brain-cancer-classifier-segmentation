{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "# modify the sys.path to include the source directory\n",
    "src_dir = pathlib.Path().absolute().parent\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from src.enums import DataSplit\n",
    "from src.utils.visualize import plot_images_and_bboxes\n",
    "from src.data.bbox import BoundingBoxDetectionDataset\n",
    "from src.enums import DataSplit\n",
    "from src.utils.transforms import BBoxResize, BBoxBaseTransform, BBoxCompose, BBoxAnchorEncode\n",
    "from src.utils.bbox import generate_anchores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_ROOT = src_dir / 'datasets'\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "transform = BBoxCompose([BBoxBaseTransform(), BBoxResize((360, 360))])\n",
    "dataset = BoundingBoxDetectionDataset(root_dir=DATASETS_ROOT, split=DataSplit.TRAIN, transform=transform)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, bboxs = next(iter(data_loader))\n",
    "plot_images_and_bboxes(images, bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, bboxs = next(iter(data_loader))\n",
    "plot_images_and_bboxes(images, bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, bboxs = next(iter(data_loader))\n",
    "plot_images_and_bboxes(images, bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0.1, 0.175, 0.25, 0.3]\n",
    "aspect_ratios = [1]\n",
    "feature_map_size = 20\n",
    "anchors, _ = generate_anchores(224, scales=scales, aspect_ratios=aspect_ratios, feature_map_size=feature_map_size)\n",
    "\n",
    "\n",
    "transform = BBoxCompose([BBoxBaseTransform(), BBoxResize((224, 244)), BBoxAnchorEncode(anchors, 0.5, 0.3)])\n",
    "dataset = BoundingBoxDetectionDataset(root_dir=DATASETS_ROOT, split=DataSplit.TRAIN, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "data_loader.dataset.image_id_to_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "images, (c, t) = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for idx, (images, (c, t)) in enumerate(data_loader):\n",
    "    if idx == 0:\n",
    "\n",
    "        images, (c, t) = next(iter(data_loader))\n",
    "        print(images)\n",
    "        non_zero_indices = torch.nonzero(c, as_tuple=True)[1]\n",
    "        print(non_zero_indices)\n",
    "        print(c[0][non_zero_indices])\n",
    "        print(t[0][non_zero_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "images, (c, t) = next(iter(data_loader))\n",
    "print(images)\n",
    "non_zero_indices = torch.nonzero(c, as_tuple=True)[1]\n",
    "print(non_zero_indices)\n",
    "print(c[0][non_zero_indices])\n",
    "t[0][non_zero_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.dataset.transform.transforms[-1].cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = BBoxCompose([BBoxBaseTransform(), BBoxResize((224, 244)), BBoxAnchorEncode(anchors, 0.5, 0.3)])\n",
    "dataset = BoundingBoxDetectionDataset(root_dir=DATASETS_ROOT, split=DataSplit.TRAIN, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "images, (c, t) = next(iter(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_indices = torch.nonzero(c, as_tuple=True)[1]\n",
    "non_zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0, non_zero_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
